{"cells":[{"cell_type":"code","source":["%run /Users/rubyhan@berkeley.edu/team28/Final_Project/Imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63ae88ec-f2e3-46e4-b401-d1d4d9b234a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class DataSplit():\n    \"Class to split data. Used as a parent class for Models\"\n    \n    def __init__(self,df, train_percent=0.8, timestamp_col='CRS_DEP_TIME_UTC', folds = 1, simple_splits = False, blind_test = None):\n        self.df = df\n        self.train_percent = train_percent\n        self.test_percent = 1 - train_percent\n        self.col_name = timestamp_col\n        self.folds = folds\n        self.simple_splits = simple_splits\n        self.train_df = []\n        self.test_df = []\n        if blind_test is not None:\n            self.test_df = [blind_test]\n            self.train_df = [DataSplit.class_balance(df, 'undersample')]\n    \n    def split_on_timestamp(self):\n        \n        min_ts = self.df.select(min(self.col_name).alias('min_ts')).take(1)[0]['min_ts']\n        max_ts = self.df.select(max(self.col_name).alias('max_ts')).take(1)[0]['max_ts']\n        time_range = (max_ts - min_ts).days\n        train_threshold_days = min_ts + dt.timedelta(days=int(self.train_percent*time_range))\n        df_train = self.df.filter(col(self.col_name) < train_threshold_days)\n        df_test = self.df.filter(col(self.col_name) >= train_threshold_days)\n        self.train_df = [df_train]\n        self.test_df = [df_test]\n\n    @staticmethod\n    def class_balance(train_df, sample = 'undersample'):\n        positive_df = train_df.filter(f.col('DEP_DEL15') == 1).cache()\n        positive_count = positive_df.count()\n        negative_df = train_df.filter(f.col('DEP_DEL15') == 0).cache()\n        negative_count = negative_df.count()\n\n        fraction = positive_count/negative_count\n\n        if sample == 'undersample':\n            train_df = negative_df.sample(withReplacement=False, \n                                          fraction=fraction, \n                                          seed = 1).unionAll(positive_df).cache()\n        elif sample == 'oversample':\n            train_df = positive_df.sample(withReplacement=True, \n                                          fraction=1/fraction, \n                                          seed = 1).unionAll(negative_df).cache()\n        return train_df    \n\n    def cv_folds(self):\n        \n        # Create simple folds by full train year, val year\n        if self.simple_splits is True:\n            for year in [2015, 2016, 2017]:\n\n                train_df = self.df.where(f.col('CRS_DEP_TIME_UTC_HOUR') == year)\n                val_df = self.df.where(f.col('CRS_DEP_TIME_UTC_HOUR') == (year + 1))\n\n                self.train_df.append(train_df)\n                self.test_df.append(val_df)\n            \n        # Create folds by number of folds, train/test split %\n        else:\n            data_size = self.df.count()\n\n            for k in range(self.folds): \n\n                # Calculate total size of each fold\n                fold_size = data_size/self.folds\n\n                # Find range of `SPLIT_ID` for train and val\n                train_ids = ((fold_size * k) + 0, \n                             int(fold_size * k + fold_size*self.train_percent))\n\n                val_ids = (train_ids[1] + 1, \n                           fold_size * k + fold_size)            \n\n                # Split the data\n                train_df = self.df.where(f.col('SPLIT_ID').between(train_ids[0], train_ids[1]))\n                val_df = self.df.where(f.col('SPLIT_ID').between(val_ids[0], val_ids[1]))\n\n                # store data\n                train_df = DataSplit.class_balance(train_df, 'undersample')\n                self.train_df.append(train_df)\n                self.test_df.append(val_df)\n\n#         return train_dfs, val_dfs\n\nclass Model(DataSplit):\n    \n#     def __init__(self,train_df, test_df, label_col=['DEP_DEL15'],feature_col=['ALL_FEATURES_VA'], **kwargs):\n    def __init__(self, df,label_col=['DEP_DEL15'],feature_col=['ALL_FEATURES_VA'], train_percent=0.8, timestamp_col='CRS_DEP_TIME_UTC', folds=1, simple_splits=False, blind_test=None):\n        \n        super().__init__(df, train_percent, timestamp_col, folds, simple_splits, blind_test)\n        self.label_col = label_col\n        self.feature_col = feature_col\n        self.model = None\n        self.best_score = 0\n        self.best_params = None\n        self.best_preds_test = None\n    \n    def get_logistic_regression(self, **kwargs):\n        model_args = kwargs\n        lr = LogisticRegression(labelCol = self.label_col[0],\n                                  featuresCol=self.feature_col[0],\n                                  elasticNetParam = model_args.get('elasticNetParam',1),\n                                  standardization = model_args.get('standardization',True),\n                                  maxIter=model_args.get('maxIter',10), \n                                  regParam=model_args.get('regParam',0.001))\n        \n        for index in range(len(self.train_df)):\n            print(f'\\n{index}-Fold:\\n')\n            self.model = lr.fit(self.train_df[index])\n            self.model.setThreshold(model_args.get('threshold',0.5))\n            self.get_predictions(index,model_args)\n    \n    def get_random_forest(self,**kwargs):\n        model_args = kwargs\n        rf = RandomForestClassifier(labelCol = self.label_col[0],\n                                    featuresCol=self.feature_col[0],\n                                    numTrees=model_args.get('numTrees',1),\n                                    maxDepth=model_args.get('maxDepth',5),\n                                    maxBins=model_args.get('maxBins',32))\n         \n        for index in range(len(self.train_df)):\n            print(f'\\n{index}-Fold:\\n')\n            self.model = rf.fit(self.train_df[index])\n            self.get_predictions(index,model_args)\n            \n    def get_GBT(self,**kwargs):\n        model_args = kwargs\n        GBT = GBTClassifier(labelCol = self.label_col[0],\n                            featuresCol=self.feature_col[0],\n                            maxIter=model_args.get('maxIter',1),\n                            maxDepth=model_args.get('maxDepth',5),\n                            maxBins=model_args.get('maxBins',32),\n                            stepSize=model_args.get('stepSize',0.1))\n         \n        for index in range(len(self.train_df)):\n            print(f'\\n{index}-Fold:\\n')\n            self.model = GBT.fit(self.train_df[index])\n            self.model.setThresholds([1-model_args.get('threshold',0.5), model_args.get('threshold',0.5)])\n            self.get_predictions(index,model_args) \n\n    \n    def get_predictions(self,index,model_args):\n        results = self.model.transform(self.test_df[index])\n        preds_train = self.model.transform(self.train_df[index])\n        self.get_metrics(preds_train,results,model_args)\n  \n    def get_confusion_matrix(self):\n        \n        pandas_df = self.best_preds_test.select([self.label_col[0],'prediction']).toPandas()\n        cf_matrix = confusion_matrix(y_true=pandas_df[self.label_col], y_pred=pandas_df['prediction'],normalize='true')\n        ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n        ax.set_title('Confusion Matrix\\n\\n');\n        ax.set_xlabel('\\nPredicted Values')\n        ax.set_ylabel('Actual Values ');\n\n        ## Ticket labels - List must be in alphabetical order\n        ax.xaxis.set_ticklabels(['False','True'])\n        ax.yaxis.set_ticklabels(['False','True'])\n        plt.show(sns)\n    \n    def get_metrics(self,preds_train, preds_test,model_args):\n        \n        train_rdd = preds_train.select(['prediction', 'DEP_DEL15']).rdd\n        train_metrics = MulticlassMetrics(train_rdd)\n        valid_rdd = preds_test.select(['prediction', 'DEP_DEL15']).rdd\n        valid_metrics = MulticlassMetrics(valid_rdd)\n        valid_score = valid_metrics.fMeasure(1.0, 0.5)\n        if valid_score > self.best_score:\n            self.best_score = valid_score\n            self.best_params = model_args\n            self.best_preds_test = preds_test\n        print(' \\t\\tTrain Metrics \\t Validation Metrics\\n')\n        print(f'Recall: \\t\\t{train_metrics.recall(label=1):.3f} \\t\\t {valid_metrics.recall(label=1):.3f}')\n        print(f'Precision: \\t\\t{train_metrics.precision(1):.3f} \\t\\t {valid_metrics.precision(1):.3f}')\n        print(f'Accuracy: \\t\\t{train_metrics.accuracy:.3f} \\t\\t {valid_metrics.accuracy:.3f}')\n        print(f'F0.5 score: \\t\\t{train_metrics.fMeasure(1.0, 0.5):.3f} \\t\\t {valid_metrics.fMeasure(1.0, 0.5):.3f}')\n        print(f'F2 score: \\t\\t{train_metrics.fMeasure(1.0, 2.0):.3f} \\t\\t {valid_metrics.fMeasure(1.0, 2.0):.3f}')\n        print(f'F1 score: \\t\\t{train_metrics.fMeasure(1.0):.3f} \\t\\t {valid_metrics.fMeasure(1.0):.3f}')\n        \n    def feature_rank(self):\n        # Only works for tree based models\n    # plot feature importance from tree models\n        feature =  self.best_preds_test.schema['ALL_FEATURES_VA'].metadata['ml_attr']['attrs']\n        dict_feature={}\n        for key in feature.keys():\n            for i in range(len(feature[key])):\n                dict_feature[feature[key][i]['idx']]= feature[key][i]['name']\n        features_df = pd.DataFrame(list(dict_feature.items()), columns = ['index', 'name'])\n        features_rank = self.model.featureImportances\n        features_rank_arr = pd.DataFrame(features_rank.toArray())\n        features_rank_arr.rename(columns={0:'score'}, inplace=True)\n\n        top_20 = features_rank_arr.sort_values('score', ascending = \n                                               False).head(20).reset_index()\n        df = pd.merge(features_df, top_20, on = 'index').sort_values('score', ascending=False)\n\n        sns.set(font_scale=1, style='whitegrid');\n        plt.subplots(figsize=(20, 15))\n        ax = sns.barplot(x = 'score', y = 'name', data = df)\n        ax.set_xlabel('Feature Importance Score')\n        ax.set_ylabel('Features')\n        ax.set_title('Feature Importance Rank');\n        plt.show(sns)\n\n    def tune_threshold(self):\n        valid_df = self.best_preds_test\n        pr_list = []\n        element1 = udf(lambda item: float(item[1]), FloatType())\n        prediction = valid_df.withColumn('prediction_prob', element1('probability'))\n        threshold_range = np.arange(start = 0.1, stop = 1.1, step = 0.1)\n        x = []\n        for i in range(1, 11):\n            x.append(f'x{i}')\n        i = 0\n        for threshold in threshold_range:\n            prediction = prediction.withColumn(x[i], f.when(\n                prediction['prediction_prob'].cast(DoubleType()) >= threshold,\n                1.0).otherwise(0.0).cast(DoubleType()))\n            i += 1    \n        for i in range(len(threshold_range) - 1):\n            preds_rdd = prediction.select([x[i], 'DEP_DEL15']).rdd\n            preds_metrics = MulticlassMetrics(preds_rdd)\n            precision = preds_metrics.precision(1)\n            recall = preds_metrics.recall(label=1)\n            f0_5 = preds_metrics.fMeasure(1.0, 0.5)\n            pr_list.append((threshold_range[i], precision, recall, f0_5))\n        return pd.DataFrame(pr_list).rename(columns = {0: 'Threshold', 1: 'Precision', \n                                                       2: 'Recall', 3: 'F0_5-Score'})\n\n    def threshold_plot(self, preds_valid_PR):\n        # plot precision-recall-f0.5 curve to tune threshold\n        sns.set(font_scale=1, style='whitegrid')\n        sns.lineplot(x='Threshold',y='Recall',data=preds_valid_PR,label='Recall')\n        sns.lineplot(x='Threshold',y='Precision',data=preds_valid_PR,label='Precision')\n        sns.lineplot(x='Threshold',y='F0_5-Score',data=preds_valid_PR,label='F0.5 Score')\n        plt.vlines(0.5, 0, 1, color='red')  \n        plt.xlabel('Threshold')\n        plt.ylabel('Score')\n        plt.title('Threshold Tuning')\n        plt.legend()\n        plt.show(sns);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b29d0dbb-9e3e-44c1-ab85-7cbd1210757c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def gridsearch(full_data, k_folds, param_list, param_names, random_shuffle_top_N=10,model_type='lr', threshold_plot=False, confusion_matrix=False, feature_rank=False, blind_test=None):\n       \n    # Assemble train/test from cv_folds\n    model = Model(df=full_data, folds = k_folds, train_percent = .8, simple_splits = False, blind_test=blind_test)\n    if blind_test is None:\n        model.cv_folds()\n      \n    # Assemble Gridsearch\n    params = list(itertools.product(*param_list))\n    random.shuffle(params)\n    # Run Models in folds\n    for param in params[:random_shuffle_top_N]:\n        \n        print(f\"\\nLooping through param:{list(zip(param_names,param))}\")\n        if model_type == 'lr':\n            model.get_logistic_regression(**{'maxIter':param[0],'elasticNetParam':param[1],'regParam':param[2], 'threshold':param[3]})\n        if model_type == 'rf':\n            model.get_random_forest(**{'numTrees':param[0],'maxDepth':param[1],'maxBins':param[2]})\n        if model_type == 'gbt':\n            model.get_GBT(**{'maxIter':param[0],'maxDepth':param[1],'maxBins':param[2],'stepSize':param[3], 'threshold':param[4]})\n    \n    if k_folds > 1:\n        print(f'\\nBest Valid f0.5 Score: {model.best_score:.3f}\\nBest Parameters: {model.best_params}')\n    \n    if threshold_plot:\n        preds_valid_PR = model.tune_threshold() # TO DO: chagne results to preds_valid\n        model.threshold_plot(preds_valid_PR)\n    \n    if feature_rank:\n        model.feature_rank()\n        \n    if confusion_matrix:\n        model.get_confusion_matrix()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d46146d3-8eb0-4d32-a627-5cea3cc1798c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"end_to_end_pipeline","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1858507102396347}},"nbformat":4,"nbformat_minor":0}
